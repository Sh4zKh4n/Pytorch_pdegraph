{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Semi-supervised classification of MNIST Dataset\n",
    "\n",
    "Less than 1.5 % labelling in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required  libs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mnist\n",
    "import torch\n",
    "import faiss\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_pdegraph.utilities import *\n",
    "from torch_pdegraph.operators import MeanCurv, GradPlusInfNorm, GradMinusInfNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a smaller dataset\n",
    "# We will take all the 70000 images and label only 100 in each class later\n",
    "images_tr = mnist.train_images()\n",
    "labels_tr = mnist.train_labels()\n",
    "images_te = mnist.test_images()\n",
    "labels_te = mnist.test_labels()\n",
    "images = np.concatenate((images_tr,images_te),axis=0)\n",
    "labels = np.concatenate((labels_tr, labels_te),axis=0)\n",
    "images_flatten = np.reshape(images, (images.shape[0], images.shape[1]*images.shape[2])).astype(np.float32) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction:\n",
    "\n",
    "- In order to simply show the effectiveness of PDEs on graph,  I am only creating a simple K-NN based graphs. This may or maynot be the best graph for a given problem at hand.\n",
    "\n",
    "- One can create graph using whatsoever apt approach or one can even use third-party network datasets and run a PDE on that graph.  PDEs are extensible to any given graph/network at hand as long as that graph has edges and weights( edge_index and edge_attr).\n",
    "\n",
    "Although torch_cluster comes with a knn-graph method. I found it to be limited and  slow when the node-features have high dimensions. We shall be using facebook's faiss library which is blazingly fast for a KNN-graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Create the intial front(level-sets) for each class with 100 labels(seeds) in each class\n",
    "\"\"\"\n",
    "Initial front creation process in literature it is \n",
    "also known as intial seed or level-set creation process.\n",
    "\"\"\"\n",
    "Front = genInitialSeeds(**dict(labels = labels, num_seeds = 100))\n",
    "\n",
    "# Create the Knn graph of the flattened image features and \n",
    "# assign weights to the edges\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.IndexFlatL2(images_flatten.shape[1])\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res,0,index)\n",
    "gpu_index_flat.add(images_flatten/1000)\n",
    "k = 30\n",
    "D, I = gpu_index_flat.search(images_flatten/1000,k+1)\n",
    "\n",
    "#Graph \n",
    "edge_index = np.vstack((I[:,1:].flatten(), np.repeat(I[:,0].flatten(),k)))\n",
    "edge_attr = np.exp(-(D[:,1:].flatten()/505000))\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).to('cuda:0')\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float32).to('cuda:0')\n",
    "edge_attr = edge_attr.view(-1,1)\n",
    "graph = Graph(edge_index, edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a manually defined PDE:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{n+1}_{i} = \\mathbf{x}^{n}_{i} + \\Delta t \\kappa_w(i,\\mathbf{x}^{n}) \\|\\nabla^{+}_w\\mathbf{x}^{n}_{i}\\|_{\\infty}, \\quad if\\quad \\kappa_w(i,\\mathbf{x}^{n}) > 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{n+1}_{i} = \\mathbf{x}^{n}_{i} + \\Delta t \\kappa_w(i,\\mathbf{x}^{n}) \\|\\nabla^{-}_w\\mathbf{x}^{n}_{i}\\|_{\\infty}, \\quad if\\quad \\kappa_w(i,\\mathbf{x}^{n}) < 0\n",
    "\\end{equation}\n",
    "\n",
    "- $\\mathbf{x}_{i}$ is the node feature/signal at the $i^{th}$ node\n",
    "- $\\nabla^{-}_{w}$, $\\nabla^{+}_{w}$ are the negative and positive directional gradients on weighted graphs respectively.\n",
    "- $\\kappa(i,\\mathbf{x})$ is the mean curvatrue operator.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from pytorch_pdegraph.operators import GradMinusInfNorm, GradPlusInfNorm, MeanCurv \n",
    "# Instantiate the operators\n",
    "ope_normM = GradMinusInfNorm.OPE(graph) \n",
    "ope_normP = GradPlusInfNorm.OPE(graph)\n",
    "ope_curv = MeanCurv.OPE(graph)\n",
    "\n",
    "# Run the above explicit PDE on intial front(level-set) on graph\n",
    "Ip = (ope_curv(fr) > 0.0)\n",
    "Im = (ope_curv(fr) < 0.0)\n",
    "fr = fr + dt * ope_curv(fr) * (ope_normP(fr) * Ip.type(torch.int) + ope_normM(fr) * Im.type(torch.int)\n",
    "```\n",
    "\n",
    "\n",
    "To know more ref [DEL11](https://ieeexplore.ieee.org/document/6116433), PDE level-set method in section 6.3 of [M. Toutain's thesis](https://tel.archives-ouvertes.fr/tel-01258738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "itr = 500\n",
    "dt = 0.05\n",
    "\n",
    "# Instantiate the operators\n",
    "ope_curv = MeanCurv.OPE(graph)\n",
    "ope_normP = GradPlusInfNorm.OPE(graph)\n",
    "ope_normM = GradMinusInfNorm.OPE(graph)\n",
    "\n",
    "#Evolved front\n",
    "newFront = []\n",
    "\n",
    "#Run the custome PDE on each initial front (initial level-set)\n",
    "for fr in tqdm(Front):\n",
    "    fr = torch.tensor(fr, dtype=torch.float32).to('cuda:0')\n",
    "    for i in range(itr):\n",
    "        Ip = (ope_curv(fr) > 0.0)\n",
    "        Im = (ope_curv(fr) < 0.0)\n",
    "        fr = fr + dt * ope_curv(fr) * (ope_normP(fr) * Ip.type(torch.int) + ope_normM(fr) * Im.type(torch.int)) \n",
    "    newfr = fr.to('cpu')\n",
    "    newFront.append(newfr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original number of elements in each class is : \n",
      " Counter({1: 7877, 7: 7293, 3: 7141, 2: 6990, 9: 6958, 0: 6903, 6: 6876, 8: 6825, 4: 6824, 5: 6313})\n",
      "The predicted number of elements are: \n",
      " Counter({1: 8618, 7: 7520, 9: 7513, 0: 7207, 6: 7075, 3: 7016, 2: 6375, 4: 6356, 5: 6331, 8: 5989})\n",
      "The matrix of confusion: \n",
      " [[6839    7    4    1    1    9   34    4    1    3]\n",
      " [   0 7825   15    1    3    2    6    9    1   15]\n",
      " [ 114  226 6290   21   25   12   34  223   30   15]\n",
      " [  23   60   28 6717    6  136    6   78   41   46]\n",
      " [   6   88    1    0 6138    0   34   10    1  546]\n",
      " [  45   44    0   75   16 5946  110    7    6   64]\n",
      " [  44   24    0    0    8   20 6779    0    1    0]\n",
      " [   4  141   11    2   25    2    0 6996    1  111]\n",
      " [  97  167   21  117   67  188   66   59 5897  146]\n",
      " [  35   36    5   82   67   16    6  134   10 6567]]\n",
      "The accuracy of the classification is: \t 94.27714285714286\n"
     ]
    }
   ],
   "source": [
    "# Now see the results\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "norm_fr = []\n",
    "\n",
    "for fr  in newFront:\n",
    "    scalar.fit(fr) \n",
    "    norm_fr.append(scalar.transform(fr))\n",
    "del(newFront)\n",
    "\n",
    "\n",
    "m = np.argmax(norm_fr, axis=0)\n",
    "m = m[:,0]\n",
    "\n",
    "import collections\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"The original number of elements in each class is : \\n {collections.Counter(labels)}\")\n",
    "print(f\"The predicted number of elements are: \\n {collections.Counter(m)}\")\n",
    "print(f\"The matrix of confusion: \\n {confusion_matrix(labels,m)}\")\n",
    "\n",
    "nmask = (m == labels)\n",
    "nmask = nmask.astype(\"int\")\n",
    "print(\"The accuracy of the classification is: \\t {acc}\".format(acc=np.sum(nmask)*100/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Here I have used a simple knn-graph construction method but with a more sophisticated method of graph creation like [two-sided tangent distance](http://www.keysers.net/daniel/files/ICPR2000.pdf) one can achieve  more than 98% of classification accuracy with 1% labeling using this PDE level-set method, as it was shown in the section 6.4.5 in the thesis of [M. Toutain](https://tel.archives-ouvertes.fr/tel-01258738)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
